{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Fundamentals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD (Resilient Distributed Dataset)\n",
    "   A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an ***Immutable***, ***partitioned collection of elements*** that can be ***operated on in parallel***.\n",
    "    \n",
    "   **Resilient**, i.e. fault-tolerant. With the help of RDD lineage graph spark is able to recompute missing or damaged partitions due to node failures.\n",
    "    \n",
    "   **Distributed** Data residing on multiple nodes in a cluster.\n",
    "    \n",
    "   **Dataset** is a collection of partitioned data with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).\n",
    "   \n",
    "\n",
    "All the work we do in spark is either create a new RDD or transform an existing RDD or call a action on an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a RDD from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(\"local\", \"PySparkWordCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"data/session_intro.txt\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic Abstract',\n",
       " '',\n",
       " 'Designing an enterprise class streaming application has significant challenges. Such challenges include handling imperfections in data stream - e.g. out of order arrival of data, handling system recoverable or irrecoverable system failures, and guaranteeing consistent results.',\n",
       " '',\n",
       " 'In real world applications these discrepancies are often exhibited and current distributed stream computation requires end user to handle such discrepancies and design custom solutions. ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and RDD using parallelize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = range(100)\n",
    "\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and --> 9\n",
      " --> 8\n",
      "of --> 7\n",
      "stream --> 5\n",
      "to --> 4\n",
      "streaming --> 4\n",
      "structured --> 3\n",
      "application --> 3\n",
      "in --> 3\n",
      "are --> 3\n",
      "end --> 3\n",
      "In --> 3\n",
      "handling --> 3\n",
      "discrepancies --> 2\n",
      "data --> 2\n",
      "consistency, --> 2\n",
      "world --> 2\n",
      "He --> 2\n",
      "real --> 2\n",
      "from --> 2\n",
      "has --> 2\n",
      "problems --> 2\n",
      "system --> 2\n",
      "on --> 2\n",
      "I --> 2\n",
      "will --> 2\n",
      "such --> 2\n",
      "Spark --> 2\n",
      "talk --> 2\n",
      "is --> 2\n",
      "out --> 1\n",
      "enterprise --> 1\n",
      "while --> 1\n",
      "challenges --> 1\n",
      "significant --> 1\n",
      "how --> 1\n",
      "high-throughput --> 1\n",
      "these --> 1\n",
      "irrecoverable --> 1\n",
      "8 --> 1\n",
      "about --> 1\n",
      "continuous --> 1\n",
      "the --> 1\n",
      "International --> 1\n",
      "try --> 1\n",
      "Topic --> 1\n",
      "Speaker --> 1\n",
      "introduced --> 1\n",
      "notion --> 1\n",
      "applications --> 1\n",
      "Institute --> 1\n",
      "arrival --> 1\n",
      "modelling, --> 1\n",
      "simplify --> 1\n",
      "BFSI --> 1\n",
      "current --> 1\n",
      "solved --> 1\n",
      "graduated --> 1\n",
      "imperfection --> 1\n",
      "fault-tolerance --> 1\n",
      "where --> 1\n",
      "explain --> 1\n",
      "custom --> 1\n",
      "queries --> 1\n",
      "include --> 1\n",
      "failures, --> 1\n",
      "transactional --> 1\n",
      "strong --> 1\n",
      "order --> 1\n",
      "provided --> 1\n",
      "processing --> 1\n",
      "component --> 1\n",
      "Designing --> 1\n",
      "handle --> 1\n",
      "design --> 1\n",
      "challenges. --> 1\n",
      "analytics --> 1\n",
      "industrial --> 1\n",
      "area --> 1\n",
      "be --> 1\n",
      "Such --> 1\n",
      "streams. --> 1\n",
      "distributed --> 1\n",
      "class --> 1\n",
      "new --> 1\n",
      "- --> 1\n",
      "interests --> 1\n",
      "source --> 1\n",
      "consistent --> 1\n",
      "nearly --> 1\n",
      "computation --> 1\n",
      "imperfections --> 1\n",
      "Bangalore. --> 1\n",
      "e.g. --> 1\n",
      "this --> 1\n",
      "working --> 1\n",
      "primary --> 1\n",
      "model --> 1\n",
      "solutions. --> 1\n",
      "Machine --> 1\n",
      "exhibited --> 1\n",
      "data, --> 1\n",
      "Information --> 1\n",
      "an --> 1\n",
      "2.0 --> 1\n",
      "2.0. --> 1\n",
      "Ajay --> 1\n",
      "Real --> 1\n",
      "domain. --> 1\n",
      "guaranteeing --> 1\n",
      "recoverable --> 1\n",
      "inbuilt. --> 1\n",
      "specific --> 1\n",
      "interactive --> 1\n",
      "development --> 1\n",
      "or --> 1\n",
      "faced --> 1\n",
      "Analytics --> 1\n",
      "requires --> 1\n",
      "learning. --> 1\n",
      "Abstract --> 1\n",
      "Technologies --> 1\n",
      "time --> 1\n",
      "Profile --> 1\n",
      "Technology, --> 1\n",
      "prefix --> 1\n",
      "Structured --> 1\n",
      "Big --> 1\n",
      "experience --> 1\n",
      "often --> 1\n",
      "building --> 1\n",
      "results. --> 1\n",
      "can --> 1\n",
      "using --> 1\n",
      "His --> 1\n",
      "currently --> 1\n",
      "mechanisms --> 1\n",
      "years --> 1\n",
      "user --> 1\n"
     ]
    }
   ],
   "source": [
    "#### Word count in Spark\n",
    "import operator\n",
    "\n",
    "#Get a RDD containing lines from this script file  \n",
    "lines = sc.textFile(data_file)\n",
    "\n",
    "#Split each line into words and assign a frequency of 1 to each word\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
    "\n",
    "#count the frequency for words\n",
    "counts = words.reduceByKey(operator.add)\n",
    "\n",
    "#Sort the counts in descending order based on the word frequency\n",
    "sorted_counts =  counts.sortBy(lambda x: x[1], False)\n",
    "\n",
    "#Get an iterator over the counts to print a word and its frequency\n",
    "for word,count in sorted_counts.toLocalIterator():\n",
    "    print(u\"{} --> {}\".format(word, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "- Filter Transformation: This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter lines which contains Spark in it\n",
    "normal_raw_data = raw_data.filter(lambda x: 'Spark' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Structured streaming is the new component introduced from Spark 2.0 to simplify end to end development of continuous application where consistency, fault-tolerance and stream imperfection handling mechanisms are inbuilt. ',\n",
       " 'In this talk I will try to explain problems faced while building high-throughput real world streaming application and how such problems can be solved using structured streaming and structured stream processing model provided in Spark 2.0. In specific I will talk about strong notion of prefix consistency, transactional source modelling, interactive stream queries and analytics on structured streams.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_raw_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count = normal_raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Map transformation:** \n",
    "    # apply a function to every element in our RDD\n",
    "\n",
    "    In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 0.069 seconds\n",
      "[['\"station_id\"', '\"bikes_available\"', '\"docks_available\"', '\"time\"'],\n",
      " ['\"2\"', '\"18\"', '\"9\"', '\"9/1/2015 00:00:02\"'],\n",
      " ['\"2\"', '\"18\"', '\"9\"', '\"9/1/2015 00:01:02\"'],\n",
      " ['\"2\"', '\"18\"', '\"9\"', '\"9/1/2015 00:02:02\"']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "data_file = \"data/status_data.csv\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))\n",
    "pprint(head_rows[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Map with Predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"2\"', ['\"2\"', '\"18\"', '\"9\"', '\"9/1/2015 00:01:02\"']),\n",
      " ('\"2\"', ['\"2\"', '\"18\"', '\"9\"', '\"9/1/2015 00:02:02\"']),\n",
      " ('\"2\"', ['\"2\"', '\"18\"', '\"9\"', '\"9/1/2015 00:03:03\"'])]\n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[0]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_line)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The collect action:\n",
    "\n",
    "So far we have used the actions count and take. Another basic action we need to learn is collect. Basically it will get all the elements in the RDD into memory for us to work with them. For this reason it has to be used with care, specially when working with large RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/apundhir/anaconda/envs/MySpark/lib/python3.5/socket.py\", line 576, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o11.setCallSite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-f3f1c06d0a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_raw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data collected in {} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msmall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_site\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apundhir/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n\u001b[1;32m    326\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o11.setCallSite"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
